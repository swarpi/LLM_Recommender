{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary modules and load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "Tokenizer Path: models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Model Path: models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\n",
      "Pipeline Parameters: {'max_length': 2048, 'num_return_sequences': 1, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.2}\n",
      "QLoRA Parameters: {'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_target_modules': ['q_proj', 'v_proj'], 'gradient_accumulation_steps': 2, 'lora_num_epochs': 2, 'lora_val_iterations': 100, 'lora_early_stopping_patience': 10, 'lora_lr': 0.0001, 'lora_micro_batch_size': 1}\n",
      "Prompt Template: {'instruction': \"### Instruction:\\n You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user‚Äôs entire history to capture deeper, stable interests that define the user‚Äôs lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user‚Äôs tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\", 'input': '{user_review}', 'output': '### Response:'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set environment variable for memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Load configuration settings\n",
    "from config import TOKENIZER_PATH, MODEL_PATH, PIPELINE_PARAMS, QLORA_PARAMS,ALPACA_LORA_PROMPTS_USER_PROFILE\n",
    "\n",
    "# Verification\n",
    "print(\"Configuration Loaded:\")\n",
    "print(\"Tokenizer Path:\", TOKENIZER_PATH)\n",
    "print(\"Model Path:\", MODEL_PATH)\n",
    "print(\"Pipeline Parameters:\", PIPELINE_PARAMS)\n",
    "print(\"QLoRA Parameters:\", QLORA_PARAMS)\n",
    "print(\"Prompt Template:\", ALPACA_LORA_PROMPTS_USER_PROFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and verify training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample: [{'User_ID': 'AFQQQ5LGNSQUEBGDCYBAZZE5T3DA', 'User_Profile': '\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\\n\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\\n* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\\n* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\\n* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\\n* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\\n* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\\n\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.', 'Candidate_Items': {'1': 'Nourishing Skin Serenity', '2': 'Botanical Beauty Discovery', '3': 'Hydrating Harmony Regimen', '4': 'Natural Radiance Revival', '5': 'Advanced Anti-Aging Adaptations'}}, {'User_ID': 'AEE4M36AZAKURLEYGV23TM3BE7OQ', 'User_Profile': '\"Short-Term Interests\": The user has recently engaged with hair care products focused on moisturizing and detangling, such as shampoos and conditioners with natural oils, hair towels, and convenient towelettes.\\n\"Long-Term Preferences\": An analysis of the user\\'s reviews reveals consistent themes:\\n* Preference for moisturizing hair products that leave fine, shoulder-length hair shiny and bouncy without greasiness\\n* Interest in products that help with detangling, reducing frizz, and minimizing static electricity\\n* Appreciation for high-quality hair accessories like twist towels and soft hair towels\\n* Values convenience items for on-the-go use, such as individually packaged towelettes\\n* Preference for natural ingredients like coconut oil and avocado oil in hair care\\n\"User_Profile\": The user appears to prioritize hair care and values products that enhance the health and appearance of their fine, shoulder-length hair. They seek moisturizing shampoos and conditioners that provide shine and manageability without weighing hair down. The user appreciates effective detangling solutions and products that combat frizz and static. They value quality hair accessories and convenient personal care items, showing a preference for natural ingredients in their hair care routine.', 'Candidate_Items': {'1': 'Anti-Frizz Hair Serums', '2': 'Leave-In Conditioners with Natural Oils', '3': 'Detangling Hair Brushes', '4': 'Travel-Friendly Dry Shampoos', '5': 'Silk Pillowcases for Hair Care'}}]\n",
      "Data Structure Verification:\n",
      "Data verification successful!\n"
     ]
    }
   ],
   "source": [
    "data_path = \"QLoRa_finetuning/new_candidate_items_with_profile.json\"\n",
    "\n",
    "# Load the training data\n",
    "with open(data_path, \"r\") as file:\n",
    "    training_data = json.load(file)\n",
    "\n",
    "# Sample a couple of data points to verify format\n",
    "print(\"Training Data Sample:\", training_data[:2])  # Display first two entries\n",
    "# Clear cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "# Verify data structure\n",
    "print(\"Data Structure Verification:\")\n",
    "for i, sample in enumerate(training_data[:2]):\n",
    "    assert \"User_ID\" in sample, f\"User_ID missing in sample {i}\"\n",
    "    assert \"User_Profile\" in sample, f\"User_Profile missing in sample {i}\"\n",
    "    assert \"Candidate_Items\" in sample, f\"Candidate_Items missing in sample {i}\"\n",
    "print(\"Data verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Tokenizer and Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS if not already set\n",
    "\n",
    "# Set 4-bit quantization configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # Use NormalFloat4 for better memory efficiency\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for more memory saving\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically maps layers to available GPU memory\n",
    ")\n",
    "# Load user reviews\n",
    "reviews_path = \"new_data/new_train_output.json\"\n",
    "with open(reviews_path, \"r\") as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Index reviews by user_id for easy matching\n",
    "reviews_by_user = {entry[\"user_id\"]: entry[\"reviews\"] for entry in reviews_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing Function to Match Reviews with Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Sample: \n",
      "### Instruction:\n",
      " You are a recommender system specialized in creating user profiles. Your task is to create a comprehensive user profile and a list of candidate items based on a user's review. The user Profile consists of: 1. Short-term Interests: Examine the user's most recent items along with their personalized descriptions 2. Long-term Preferences: Analyze all reviews and items from the user‚Äôs entire history to capture deeper, stable interests that define the user‚Äôs lasting preferences. Use this comprehensive historical data to outline consistent themes and inclinations that have remained steady over time. 3. User Profile Summary: Synthesize these findings into a concise profile (max 200 words) that combines insights from both short-term interests and long-term preferences. Use the short-term interests as a query to refine or highlight relevant themes from the long-term history, and provide a cohesive picture of the user‚Äôs tastes, typical habits, and potential future interests.Present your analysis under clear section headings.Do Not include any Code in your response. Think step by step\n",
      "\n",
      "### Input:\n",
      "User reviews:\n",
      "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
      "Rating: 5.0\n",
      "Title: Intense Hydration\n",
      "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
      "Rating: 5.0\n",
      "Title: Youth Serum\n",
      "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
      "Rating: 5.0\n",
      "Title: Doesn't Leave Skin Sticky\n",
      "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
      "Rating: 5.0\n",
      "Title: Special Care Mask\n",
      "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
      "Rating: 5.0\n",
      "Title: Lightweight and Creamy\n",
      "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
      "Rating: 5.0\n",
      "Title: New Favorite!!\n",
      "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
      "Rating: 5.0\n",
      "Title: Effective Mask\n",
      "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "Product: 13.5 fl. Oz. Goat Milk Facial Cleanser, Moisturizing Face Wash for Women, Hydrating Natural Face Wash, Anti-aging Face Wash, Face Wash for Aging Ski\n",
      "Rating: 5.0\n",
      "Title: Gentle & Natural\n",
      "Review: I'm trying to do a better job with paying as much attention to the ingredients in my skincare as those in my food. This is a great natural cleanser. It's very gentle and is good for sensitive skin. The lactic acid hydrates and gently exfoliates over time. You won't need a science degree to understand the majority of the ingredients. This is a great clean beauty cleanser that I highly recommend.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Short-Term Interests\": The user recently reviewed products focused on facial skincare, specifically creams and serums targeting anti-aging concerns such as fine lines, wrinkles, and dehydration. They showed interest in natural and organic ingredients, vegan-friendly options, and cruelty-free practices. Their preference leans towards face washes and moisturizers that cater to specific skin concerns like acne-prone, sensitive, and dry skin.\n",
      "\"Long-Term Preferences\": Based on the entirety of their reviews, some common denominators emerge:\n",
      "* Focus on natural ingredients, particularly plant-based extracts, essential oils, and herbal remedies\n",
      "* Concern for anti-aging and rejuvenation, seeking effective solutions for maintaining healthy-looking skin\n",
      "* Interest in sheet masks, especially those with unique features like eye patches and targeted application areas\n",
      "* Appreciation for moisturized and hydrated skin, often mentioning specific requirements like deep hydration and non-sticky textures\n",
      "* Willingness to explore various brands and products, demonstrating adaptability and open-mindedness\n",
      "\"User_Profile\": Our user appears to be someone who prioritizes natural and organic approaches to skincare while focusing on addressing specific skin concerns. They exhibit a willingness to experiment with diverse products and techniques to achieve optimal results. As they continue exploring the world of skincare, they may gravitate toward more niche markets, such as bespoke formulas tailored to individual skin types or advanced technology-driven treatments. For now, our recommendation would focus on recommending products featuring natural, sustainable, and innovative formulations catering to their varied skin needs, including hydration, anti-aging, and sensitivity management. A suggested next step could involve introducing them to emerging trends in customized skincare regimens and cutting-edge technologies for enhanced customer experience.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(profile_sample):\n",
    "    user_id = profile_sample[\"User_ID\"]\n",
    "    reviews = reviews_by_user.get(user_id, [])\n",
    "    review_texts = [f\"Product: {review['product_name']}\\nRating: {review['rating']}\\nTitle: {review['title']}\\nReview: {review['text']}\\n\" \n",
    "                    for review in reviews]\n",
    "    review_texts = review_texts[-10:]\n",
    "    formatted_reviews = \"\\n\".join(review_texts)\n",
    "    \n",
    "    instruction = ALPACA_LORA_PROMPTS_USER_PROFILE['instruction']\n",
    "    input_text = f\"User reviews:\\n{formatted_reviews}\" if formatted_reviews else \"No reviews available for this user.\"\n",
    "    output_text = profile_sample[\"User_Profile\"]\n",
    "    \n",
    "    full_text = f\"\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output_text}\"\n",
    "    return full_text\n",
    "\n",
    "# Verify preprocessing\n",
    "print(\"Preprocessed Sample:\", preprocess_function(training_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(sample):\n",
    "    processed_text = preprocess_function(sample)\n",
    "    tokenized = tokenizer(\n",
    "        processed_text,\n",
    "        truncation=True,\n",
    "        max_length=PIPELINE_PARAMS['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # Set labels identical to input_ids\n",
    "    return tokenized\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=QLORA_PARAMS['lora_r'],\n",
    "    lora_alpha=QLORA_PARAMS['lora_alpha'],\n",
    "    lora_dropout=QLORA_PARAMS['lora_dropout'],\n",
    "    target_modules=QLORA_PARAMS['lora_target_modules'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure LoRA and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 32 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà‚ñè      | 10/32 [07:04<16:27, 44.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4053, 'grad_norm': 17.300838470458984, 'learning_rate': 7.8125e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 20/32 [16:03<10:47, 53.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4451, 'grad_norm': 4.829381942749023, 'learning_rate': 5e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 30/32 [25:00<01:48, 54.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4705, 'grad_norm': 0.9115433692932129, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [26:49<00:00, 50.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1609.739, 'train_samples_per_second': 0.04, 'train_steps_per_second': 0.02, 'train_loss': 3.0273856967687607, 'epoch': 2.0}\n",
      "Model trained with 32 samples saved to outputs/adapter_test_user_profile_epoch_2_32_samples\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "batch_size = 16\n",
    "\n",
    "# Training sizes\n",
    "training_sizes = [32]\n",
    "\n",
    "# Loop through different training sizes\n",
    "for train_size in training_sizes:\n",
    "    # Split the dataset\n",
    "    train_data = training_data[:train_size]\n",
    "    eval_data = training_data[train_size:train_size + int(0.2 * train_size)]  # 20% of training data for evaluation\n",
    "\n",
    "    # Tokenize datasets\n",
    "    train_tokenized_data = [tokenize_function(sample) for sample in train_data]\n",
    "    eval_tokenized_data = [tokenize_function(sample) for sample in eval_data]\n",
    "\n",
    "    # Convert tokenized data to Dataset format\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in train_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in train_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in train_tokenized_data]\n",
    "    })\n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"input_ids\": [x[\"input_ids\"][0] for x in eval_tokenized_data],\n",
    "        \"attention_mask\": [x[\"attention_mask\"][0] for x in eval_tokenized_data],\n",
    "        \"labels\": [x[\"labels\"][0] for x in eval_tokenized_data]\n",
    "    })\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=QLORA_PARAMS['gradient_accumulation_steps'],\n",
    "        num_train_epochs=QLORA_PARAMS['lora_num_epochs'],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        save_steps=QLORA_PARAMS['lora_val_iterations'],\n",
    "        logging_steps=10,\n",
    "        learning_rate=QLORA_PARAMS['lora_lr'],\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, padding=True)\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Clear GPU cache before training\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Start training\n",
    "    print(f\"Starting training with {train_size} samples.\")\n",
    "    trainer.train()\n",
    "    adapter_name = f\"adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\"\n",
    "    # Save the model and tokenizer in separate directories for each training size\n",
    "    model.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    tokenizer.save_pretrained(f\"outputs/{adapter_name}\")\n",
    "    print(f\"Model trained with {train_size} samples saved to outputs/{adapter_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 1056.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#quantization_config=bnb_config,\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#torch_dtype=torch.float16,\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the user profile adapter\u001b[39;00m\n\u001b[0;32m     26\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# Adjust based on your adapter's training size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\modeling_utils.py:4174\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4171\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4174\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4177\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\Trung\\anaconda3\\envs\\torch_recommender\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from config import *\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/hf-frompretrained-download/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# Load the base model and tokenizer\n",
    "base_model_path =\"models/hf-frompretrained-downloadmeta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the user profile adapter\n",
    "train_size = 64  # Adjust based on your adapter's training size\n",
    "adapter_path_user_profile =f\"outputs/adapter_test_user_profile_epoch_{QLORA_PARAMS['lora_num_epochs']}_{train_size}_samples\"\n",
    "adapter_name_user_profile = \"user_profile\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path_user_profile, adapter_name=adapter_name_user_profile)\n",
    "\n",
    "print(f\"Loaded user profile adapter: {adapter_name_user_profile}\")\n",
    "\n",
    "# Load the candidate items adapter\n",
    "adapter_path_candidate_items = f\"outputs/adapter_test_candidate_items_epoch_{QLORA_PARAMS['lora_num_epochs']}_{64}_samples\"\n",
    "adapter_name_candidate_items = \"candidate_items\"\n",
    "model.load_adapter(adapter_path_candidate_items, adapter_name=adapter_name_candidate_items)\n",
    "\n",
    "print(f\"Loaded candidate items adapter: {adapter_name_candidate_items}\")\n",
    "\n",
    "# Print the list of adapters loaded into the model\n",
    "print(\"Adapters loaded in the model:\", list(model.peft_config.keys()))\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the text generation function\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=PIPELINE_PARAMS['max_length'],\n",
    "        do_sample=True,\n",
    "        temperature=PIPELINE_PARAMS['temperature'],\n",
    "        top_k=PIPELINE_PARAMS['top_k'],\n",
    "        top_p=PIPELINE_PARAMS['top_p'],\n",
    "        repetition_penalty=PIPELINE_PARAMS['repetition_penalty'],\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate user profile\n",
    "user_reviews = \"\"\"User reviews:\n",
    "Product: Moisturizing Facial Emulsion for Restoring Hydrating Smoothing Skin from Manilla Natural Skincare\n",
    "Rating: 5.0\n",
    "Title: Intense Hydration\n",
    "Review: With winter approaching, it's time to start thinking about cold-weather skincare. This moisturizer is a good choice because it's a blend of several skin-loving oils like argan, coconut and macadamia. It's fairly lightweight but it protects and hydrates like a heavier cream. It can be used both day and night. The packaging is pretty enough for display. It can be used by all skin types. I highly recommend this product and the entire line from this brand.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Dr. Denese SkinScience Essential Lipid Anti Aging Power Infusion Dry Oil - Skin Nutrients 97% Organic 100% Natural - Rejuvinating Blend with Amaranth & Resveratrol - Paraben-Free, Cruelty-Free - 2oz\n",
    "Rating: 5.0\n",
    "Title: Youth Serum\n",
    "Review: One of the reasons skin starts to wrinkle as we age is that we make less and less of the natural oil that keeps skin looking youthful. This serum is a good way to supplement what your skin lacks. The oils in this blend are all high quality. The texture is lightweight and easily absorbed. Dr. Denese is a trusted brand when it comes to care for aging skin. Whether you're trying to slow the clock or reverse it a little, this serum should be part of your daily skincare routine. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: [Abib] Creme coating mask Tone-up solution 17g (5pcs)\n",
    "Rating: 5.0\n",
    "Title: Doesn't Leave Skin Sticky\n",
    "Review: I use lots of sheet masks. Most leave your skin a little sticky when the serum dries. This mask works differently. The sheet locks the moisture in and doesn't leave a sticky residue. It's a great hydration and brightening mask. I highly recommend it.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Easydew 2-Step Face Contour Sheet Mask - Tightening & Anti-Aging & Hydrating Korean Face Mask with DW-EGF, Hyaluronic Acid for Anti-Winkle, Reducing Fine Line & Producing Collagen (Qty 5)\n",
    "Rating: 5.0\n",
    "Title: Special Care Mask\n",
    "Review: I really like that this mask includes special patches for the eyes and smile lines. If you're over 30, you'll appreciate the extra help. You apply the patches first, the the sheet mask. I keep all of my sheet masks in the refrigerator. You'll appreciate the cooling and anti-inflammatory effect if you do so with this mask. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Higher Education Skincare: Goal Digger - Silky Moisturizing Cucumber Creme; formulated for dry and sensitive skin; cucumber extract; natural extract: melon, kale, cabbage, ginger, turmeric - 1.7 fl oz\n",
    "Rating: 5.0\n",
    "Title: Lightweight and Creamy\n",
    "Review: I really like this moisturizer. It's lightweight and creamy. It has a light cucumber scent that I find refreshing. It's a powerhouse at deeply hydrating parched skin. It also locks in moisture to keep skin looking fresh. It contains botanicals that soothe irritated or inflamed skin. It's ideal for those with acne, rosacea or eczema. You can use it as both a day and night cream. I highly recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: Queendom Unlashed Mascara | Volumizing and Lengthening | Boosts Lash Length | Vegan, Cruelty Free, Paraben Free\n",
    "Rating: 5.0\n",
    "Title: New Favorite!!\n",
    "Review: Every mascara claims to be new and different. This one actually lives up to the hype. The fibers have a novel shape that really makes your lashes POP. If you're not a fan of falsies, this is a great alternative. The formula isn't waterproof, so keep that in mind. I HIGHLY recommend this product.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\n",
    "Product: ABOUT ME MediAnswer Collagen Firming Up Mask 4ea (1box) 25g - 77% Pure Collagen Extract Anti-Aging Total Care Facial Mask Sheet, Powerful Hydrating and Anti-Wrinkle Night Skin Care\n",
    "Rating: 5.0\n",
    "Title: Effective Mask\n",
    "Review: If you're reading this review, you probably already know the anti-aging benefits of collagen. This mask has a high-potency form of collagen along with a special delivery system to help firm your skin. The mask is in two parts, so you can get a better fit than a regular sheet mask. I can't say it performs miracles, but it's definitely worth a try.<br /><br />Thanks so much for taking the time to read this review. I hope the information that I've provided makes your decision a little easier.\n",
    "\"\"\"\n",
    "\n",
    "prompt_user_profile = (\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_USER_PROFILE['input'].replace(\"{user_review}\", user_reviews)+ \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to user profile adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_user_profile}\")\n",
    "model.set_adapter(adapter_name_user_profile)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_profile = generate_text(prompt_user_profile)\n",
    "generated_profile = generated_profile[len(prompt_user_profile):].strip()\n",
    "print(\"\\nGenerated User Profile:\")\n",
    "print(generated_profile)\n",
    "\n",
    "# Generate candidate items using the generated user profile\n",
    "prompt_candidate_items = (\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['instruction'] + \"\\n\\n\" +\n",
    "    ALPACA_LORA_PROMPTS_CANDIDATE_ITEMS['input'].replace(\"{user_profile}\", generated_profile) + \"\\n### Response\"\n",
    ")\n",
    "\n",
    "# Set the active adapter to candidate items adapter\n",
    "print(f\"\\nSetting active adapter to: {adapter_name_candidate_items}\")\n",
    "model.set_adapter(adapter_name_candidate_items)\n",
    "print(f\"Current active adapter: {model.active_adapter}\")\n",
    "\n",
    "generated_items = generate_text(prompt_candidate_items)\n",
    "generated_items = generated_items[len(prompt_candidate_items):].strip()\n",
    "print(\"\\nGenerated Candidate Items:\")\n",
    "print(generated_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
